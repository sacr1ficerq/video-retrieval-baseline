{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сбор датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено 999 видео до 30 секунд\n",
      "Скачиваю архив...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Извлечение: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 999/999 [00:00<00:00, 1946.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработано 999 видео\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_path</th>\n",
       "      <th>caption</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSRVTT_videos/video9778.mp4</td>\n",
       "      <td>a little boy singing in front of judges and crowd</td>\n",
       "      <td>kids/family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSRVTT_videos/video9832.mp4</td>\n",
       "      <td>a video game character rides around on a motor...</td>\n",
       "      <td>vehicles/autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MSRVTT_videos/video7767.mp4</td>\n",
       "      <td>a slideshow with captions</td>\n",
       "      <td>music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MSRVTT_videos/video7369.mp4</td>\n",
       "      <td>a man is talking about opening a laptop case</td>\n",
       "      <td>howto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MSRVTT_videos/video9731.mp4</td>\n",
       "      <td>a woman is mixing food in a mixing bowl</td>\n",
       "      <td>cooking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>MSRVTT_videos/video8814.mp4</td>\n",
       "      <td>the judges make a decision</td>\n",
       "      <td>kids/family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>MSRVTT_videos/video9827.mp4</td>\n",
       "      <td>lady gaga sings in a music video</td>\n",
       "      <td>music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>MSRVTT_videos/video9815.mp4</td>\n",
       "      <td>a mashup of music videos is being played</td>\n",
       "      <td>music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>MSRVTT_videos/video8901.mp4</td>\n",
       "      <td>men pushing a car down assembly line</td>\n",
       "      <td>vehicles/autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>MSRVTT_videos/video8322.mp4</td>\n",
       "      <td>a lady tries to ride on bicycle but fails</td>\n",
       "      <td>vehicles/autos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      video_path  \\\n",
       "0    MSRVTT_videos/video9778.mp4   \n",
       "1    MSRVTT_videos/video9832.mp4   \n",
       "2    MSRVTT_videos/video7767.mp4   \n",
       "3    MSRVTT_videos/video7369.mp4   \n",
       "4    MSRVTT_videos/video9731.mp4   \n",
       "..                           ...   \n",
       "994  MSRVTT_videos/video8814.mp4   \n",
       "995  MSRVTT_videos/video9827.mp4   \n",
       "996  MSRVTT_videos/video9815.mp4   \n",
       "997  MSRVTT_videos/video8901.mp4   \n",
       "998  MSRVTT_videos/video8322.mp4   \n",
       "\n",
       "                                               caption        category  \n",
       "0    a little boy singing in front of judges and crowd     kids/family  \n",
       "1    a video game character rides around on a motor...  vehicles/autos  \n",
       "2                            a slideshow with captions           music  \n",
       "3         a man is talking about opening a laptop case           howto  \n",
       "4              a woman is mixing food in a mixing bowl         cooking  \n",
       "..                                                 ...             ...  \n",
       "994                         the judges make a decision     kids/family  \n",
       "995                   lady gaga sings in a music video           music  \n",
       "996           a mashup of music videos is being played           music  \n",
       "997               men pushing a car down assembly line  vehicles/autos  \n",
       "998          a lady tries to ride on bicycle but fails  vehicles/autos  \n",
       "\n",
       "[999 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import subprocess\n",
    "import os\n",
    "import zipfile\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "from datasets import load_dataset\n",
    "\n",
    "category_map = {\n",
    "    \"0\": \"music\",\n",
    "    \"1\": \"people\", \n",
    "    \"2\": \"gaming\",\n",
    "    \"3\": \"sports/actions\",\n",
    "    \"4\": \"news/events/politics\",\n",
    "    \"5\": \"education\",\n",
    "    \"6\": \"tv shows\",\n",
    "    \"7\": \"movie/comedy\", \n",
    "    \"8\": \"animation\",\n",
    "    \"9\": \"vehicles/autos\",\n",
    "    \"10\": \"howto\",\n",
    "    \"11\": \"travel\",\n",
    "    \"12\": \"science/technology\",\n",
    "    \"13\": \"animals/pets\",\n",
    "    \"14\": \"kids/family\",\n",
    "    \"15\": \"documentary\",\n",
    "    \"16\": \"food/drink\",\n",
    "    \"17\": \"cooking\",\n",
    "    \"18\": \"beauty/fashion\",\n",
    "    \"19\": \"advertisement\"\n",
    "}\n",
    "\n",
    "def download_msrvtt_zip():\n",
    "    zip_path = \"MSRVTT_Videos.zip\"\n",
    "    if not os.path.exists(zip_path):\n",
    "        url = \"https://huggingface.co/datasets/friedrichor/MSR-VTT/resolve/main/MSRVTT_Videos.zip\"\n",
    "        print(\"Скачиваю архив...\")\n",
    "        response = requests.get(url)\n",
    "        with open(zip_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    return zip_path\n",
    "\n",
    "def extract_video(zip_ref, video_file, output_dir):\n",
    "    video_path = os.path.join(output_dir, video_file.split('/')[-1])\n",
    "    if os.path.exists(video_path):\n",
    "        return video_path\n",
    "    \n",
    "    try:\n",
    "        with zip_ref.open(f\"video/{video_file}\") as src, open(video_path, 'wb') as dst:\n",
    "            dst.write(src.read())\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    return video_path\n",
    "\n",
    "def process_msrvtt_video(zip_ref, item, output_dir):\n",
    "    video_file = item['video']\n",
    "    duration = item['end time'] - item['start time']\n",
    "    \n",
    "    if duration > 30:\n",
    "        return None\n",
    "    \n",
    "    video_path = extract_video(zip_ref, video_file, output_dir)\n",
    "    if not video_path:\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        'video_path': video_path,\n",
    "        'caption': item['caption'],\n",
    "        'category': category_map.get(str(item['category']), \"unknown\")\n",
    "    }\n",
    "\n",
    "def download_msrvtt_dataset(save_path, num_videos, max_workers=10):\n",
    "    dataset = load_dataset(\"friedrichor/MSR-VTT\", \"test_1k\", split=\"test\", streaming=True).shuffle(seed=42, buffer_size=1000)\n",
    "    \n",
    "    items = []\n",
    "    for i, item in enumerate(dataset):\n",
    "        if item['end time'] - item['start time'] <= 30:\n",
    "            items.append(item)\n",
    "        if len(items) >= num_videos:\n",
    "            break\n",
    "    \n",
    "    print(f\"Найдено {len(items)} видео до 30 секунд\")\n",
    "    \n",
    "    zip_path = download_msrvtt_zip()\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    samples = []\n",
    "    lock = threading.Lock()\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref, ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_msrvtt_video, zip_ref, item, save_path): item for item in items}\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Извлечение\"):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                with lock:\n",
    "                    samples.append(result)\n",
    "    \n",
    "    df = pd.DataFrame(samples)\n",
    "    df.to_pickle(os.path.join(save_path, \"msrvtt_dataset.pkl\"))\n",
    "    print(f\"Обработано {len(df)} видео\")\n",
    "    return df\n",
    "\n",
    "download_msrvtt_dataset(\"MSRVTT_videos\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [03:06<00:00,  5.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработано 0 видео\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import subprocess\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import tempfile\n",
    "import threading\n",
    "from datasets import load_dataset\n",
    "import queue\n",
    "\n",
    "def download_video(video_id):\n",
    "    url = f\"https://huggingface.co/datasets/VLM2Vec/VATEX/resolve/main/raw_videos/{video_id}.mp4\"\n",
    "    with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as tmp:\n",
    "        tmp_path = tmp.name\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            if response.status_code != 200:\n",
    "                return None\n",
    "            tmp.write(response.content)\n",
    "            return tmp_path\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "def cut_video(temp_path, video_id, output_dir):\n",
    "    parts = video_id.split('_')\n",
    "    start_time = int(parts[-2])\n",
    "    end_time = int(parts[-1])\n",
    "    cut_path = os.path.join(output_dir, f\"{video_id}.mp4\")\n",
    "    \n",
    "    if os.path.exists(cut_path):\n",
    "        os.unlink(temp_path)\n",
    "        return cut_path\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run([\n",
    "            'ffmpeg',\n",
    "            '-ss', str(start_time),\n",
    "            '-i', temp_path,\n",
    "            '-t', str(end_time - start_time),\n",
    "            '-c:v', 'libx264',\n",
    "            '-preset', 'ultrafast',\n",
    "            '-c:a', 'aac',\n",
    "            '-y',\n",
    "            '-loglevel', 'error',\n",
    "            cut_path\n",
    "        ], capture_output=True, timeout=30)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    os.unlink(temp_path)\n",
    "    return cut_path if result.returncode == 0 else None\n",
    "\n",
    "def process_video(item, output_dir):\n",
    "    video_id = item['videoID']\n",
    "    temp_path = download_video(video_id)\n",
    "    if not temp_path:\n",
    "        return None\n",
    "    return cut_video(temp_path, video_id, output_dir)\n",
    "\n",
    "def download_vatex_dataset(save_path, num_videos, max_workers=10):\n",
    "    dataset = load_dataset(\"VLM2Vec/VATEX\", \"vatex_test\", split=\"test\", streaming=True).shuffle(seed=42, buffer_size=1000)\n",
    "    items = []\n",
    "    for i, item in enumerate(dataset):\n",
    "        if i >= num_videos:\n",
    "            break\n",
    "        items.append(item)\n",
    "    \n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    samples = []\n",
    "    lock = threading.Lock()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_video, item, save_path): item for item in items}\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Обработка\"):\n",
    "            video_path = future.result()\n",
    "            if video_path:\n",
    "                item = futures[future]\n",
    "                with lock:\n",
    "                    samples.append({\n",
    "                        'video_path': video_path,\n",
    "                        'caption': item['enCap'],\n",
    "                        'category': \"people\"\n",
    "                    })\n",
    "    \n",
    "    df = pd.DataFrame(samples)\n",
    "    df.to_pickle(os.path.join(save_path, \"vatex_dataset.pkl\"))\n",
    "    print(f\"Обработано {len(df)} видео\")\n",
    "    return df\n",
    "\n",
    "download_vatex_dataset(\"VATEX_videos\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено 1000 видео до 30 секунд\n",
      "Скачиваю архив...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Извлечение: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1674.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработано 1000 видео\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_path</th>\n",
       "      <th>caption</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouCook2_videos/sGzBQrg1adY_9.mp4</td>\n",
       "      <td>add marsala powder</td>\n",
       "      <td>cooking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YouCook2_videos/pNAwkqm4t3A_5.mp4</td>\n",
       "      <td>remove onion rings from the pot</td>\n",
       "      <td>cooking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YouCook2_videos/F564e476ULM_7.mp4</td>\n",
       "      <td>place the lobster on a towel to dry off</td>\n",
       "      <td>cooking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YouCook2_videos/InDwfZmSikI_3.mp4</td>\n",
       "      <td>season the pizza with sea salt and basil</td>\n",
       "      <td>cooking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YouCook2_videos/eQZEf3NCCo4_4.mp4</td>\n",
       "      <td>place the seaweed down and put the rice on it</td>\n",
       "      <td>cooking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>YouCook2_videos/mV3m2svj3XE_1.mp4</td>\n",
       "      <td>slice chilis and a lime and add the pieces and...</td>\n",
       "      <td>cooking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>YouCook2_videos/4apR0YypAGc_2.mp4</td>\n",
       "      <td>add some udon noodles to the broth</td>\n",
       "      <td>cooking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>YouCook2_videos/Nbh64ntT3EM_5.mp4</td>\n",
       "      <td>spread some parmesan cheese and stir the egg m...</td>\n",
       "      <td>cooking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>YouCook2_videos/G-AUY-jWzck_3.mp4</td>\n",
       "      <td>add pepper and sauerkraut to the pot</td>\n",
       "      <td>cooking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>YouCook2_videos/JPbFE731Y0c_2.mp4</td>\n",
       "      <td>place the bratwurst on the grill and cook them...</td>\n",
       "      <td>cooking</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            video_path  \\\n",
       "0    YouCook2_videos/sGzBQrg1adY_9.mp4   \n",
       "1    YouCook2_videos/pNAwkqm4t3A_5.mp4   \n",
       "2    YouCook2_videos/F564e476ULM_7.mp4   \n",
       "3    YouCook2_videos/InDwfZmSikI_3.mp4   \n",
       "4    YouCook2_videos/eQZEf3NCCo4_4.mp4   \n",
       "..                                 ...   \n",
       "995  YouCook2_videos/mV3m2svj3XE_1.mp4   \n",
       "996  YouCook2_videos/4apR0YypAGc_2.mp4   \n",
       "997  YouCook2_videos/Nbh64ntT3EM_5.mp4   \n",
       "998  YouCook2_videos/G-AUY-jWzck_3.mp4   \n",
       "999  YouCook2_videos/JPbFE731Y0c_2.mp4   \n",
       "\n",
       "                                               caption category  \n",
       "0                                   add marsala powder  cooking  \n",
       "1                      remove onion rings from the pot  cooking  \n",
       "2              place the lobster on a towel to dry off  cooking  \n",
       "3             season the pizza with sea salt and basil  cooking  \n",
       "4        place the seaweed down and put the rice on it  cooking  \n",
       "..                                                 ...      ...  \n",
       "995  slice chilis and a lime and add the pieces and...  cooking  \n",
       "996                 add some udon noodles to the broth  cooking  \n",
       "997  spread some parmesan cheese and stir the egg m...  cooking  \n",
       "998               add pepper and sauerkraut to the pot  cooking  \n",
       "999  place the bratwurst on the grill and cook them...  cooking  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import zipfile\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "from datasets import load_dataset\n",
    "\n",
    "def download_youcook2_zip():\n",
    "    zip_path = \"YouCookIIVideos.zip\"\n",
    "    url = \"https://huggingface.co/datasets/lmms-lab/YouCook2/resolve/main/YouCookIIVideos.zip\"\n",
    "    \n",
    "    if not os.path.exists(zip_path):\n",
    "        print(\"Скачиваю архив...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(zip_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "    return zip_path\n",
    "\n",
    "def extract_video(zip_ref, video_path, output_dir):\n",
    "    full_video_path = f\"YouCookIIVideos/{video_path}\"\n",
    "    filename = os.path.basename(video_path)\n",
    "    save_path = os.path.join(output_dir, filename)\n",
    "    \n",
    "    if os.path.exists(save_path):\n",
    "        return save_path\n",
    "    \n",
    "    try:\n",
    "        with zip_ref.open(full_video_path) as src, open(save_path, 'wb') as dst:\n",
    "            dst.write(src.read())\n",
    "        return save_path\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def process_youcook2_video(zip_ref, item, output_dir):\n",
    "    video_path = item['video_path']\n",
    "    \n",
    "    if not video_path.endswith('.mp4'):\n",
    "        return None\n",
    "    \n",
    "    start, end = item['segment']\n",
    "    if end - start > 30:\n",
    "        return None\n",
    "    \n",
    "    extracted_path = extract_video(zip_ref, video_path, output_dir)\n",
    "    if not extracted_path:\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        'video_path': extracted_path,\n",
    "        'caption': item['sentence'],\n",
    "        'category': \"cooking\"\n",
    "    }\n",
    "\n",
    "def download_youcook2_dataset(save_path, num_videos, split=\"val\", max_workers=10):\n",
    "    dataset = load_dataset(\"lmms-lab/YouCook2\", split=split, streaming=True).shuffle(seed=42, buffer_size=1000)\n",
    "    \n",
    "    items = []\n",
    "    for i, item in enumerate(dataset):\n",
    "        if not item['video_path'].endswith('.mp4'):\n",
    "            continue\n",
    "        \n",
    "        start, end = item['segment']\n",
    "        if end - start <= 30:\n",
    "            items.append(item)\n",
    "        \n",
    "        if len(items) >= num_videos:\n",
    "            break\n",
    "    \n",
    "    print(f\"Найдено {len(items)} видео до 30 секунд\")\n",
    "    \n",
    "    zip_path = download_youcook2_zip()\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    samples = []\n",
    "    lock = threading.Lock()\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref, ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_youcook2_video, zip_ref, item, save_path): item for item in items}\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Извлечение\"):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                with lock:\n",
    "                    samples.append(result)\n",
    "    \n",
    "    df = pd.DataFrame(samples)\n",
    "    df.to_pickle(os.path.join(save_path, \"youcook2_dataset.pkl\"))\n",
    "    print(f\"Обработано {len(df)} видео\")\n",
    "    return df\n",
    "\n",
    "download_youcook2_dataset(\"YouCook2_videos\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Получение CLIP-эмбеддингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install av einops timm protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:6\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:6\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import av\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    AutoProcessor, AutoModel, AutoConfig, AutoTokenizer,\n",
    "    CLIPImageProcessor, CLIPTokenizer\n",
    ")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class UniversalVideoModel:\n",
    "    def __init__(self, model_type=\"xclip\", num_frames=8):\n",
    "        self.model_type = model_type.lower()\n",
    "        self.num_frames = num_frames\n",
    "        self.device = DEVICE\n",
    "        \n",
    "        print(f\"Загрузка модели: {self.model_type.upper()}...\")\n",
    "        \n",
    "        if self.model_type == \"xclip\":\n",
    "            model_id = \"microsoft/xclip-large-patch14\" \n",
    "            self.processor = AutoProcessor.from_pretrained(model_id)\n",
    "            self.model = AutoModel.from_pretrained(model_id).to(self.device)\n",
    "        \n",
    "        elif self.model_type == \"siglip\":\n",
    "            model_id = \"google/siglip-so400m-patch14-384\"\n",
    "            self.processor = AutoProcessor.from_pretrained(model_id)\n",
    "            self.model = AutoModel.from_pretrained(model_id).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        print(\"Модель загружена\")\n",
    "\n",
    "    def _get_video_frames(self, video_path):\n",
    "        try:\n",
    "            container = av.open(video_path)\n",
    "            total_frames = container.streams.video[0].frames\n",
    "            if total_frames == 0: total_frames = 100\n",
    "            \n",
    "            indices = np.linspace(0, total_frames - 1, self.num_frames).astype(int)\n",
    "            \n",
    "            frames = []\n",
    "            container.seek(0)\n",
    "            for i, frame in enumerate(container.decode(video=0)):\n",
    "                if i in indices:\n",
    "                    frames.append(frame.to_image().convert(\"RGB\"))\n",
    "                    if len(frames) >= self.num_frames:\n",
    "                        break\n",
    "            \n",
    "            if frames and len(frames) < self.num_frames:\n",
    "                while len(frames) < self.num_frames:\n",
    "                    frames.append(frames[-1])\n",
    "                    \n",
    "            return frames if len(frames) > 0 else None\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "    def encode(self, video_path, text):\n",
    "        frames = self._get_video_frames(video_path)\n",
    "        if frames is None:\n",
    "            return None, None\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            if self.model_type == \"xclip\":\n",
    "                inputs = self.processor(\n",
    "                    text=[text], \n",
    "                    videos=list(frames), \n",
    "                    return_tensors=\"pt\", \n",
    "                    padding=True,\n",
    "                ).to(self.device)\n",
    "                \n",
    "                outputs = self.model(**inputs, interpolate_pos_encoding=True)\n",
    "                \n",
    "                v_emb = outputs.video_embeds.cpu().numpy()\n",
    "                t_emb = outputs.text_embeds.reshape(1, -1).cpu().numpy()\n",
    "                \n",
    "            elif self.model_type == \"siglip\":\n",
    "                inputs_text = self.processor(text=[text], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64).to(self.device)\n",
    "                t_feat = self.model.get_text_features(**inputs_text)\n",
    "                \n",
    "                inputs_video = self.processor(images=frames, return_tensors=\"pt\").to(self.device)\n",
    "                frame_feats = self.model.get_image_features(**inputs_video)\n",
    "                \n",
    "                v_feat = torch.mean(frame_feats, dim=0, keepdim=True)\n",
    "                \n",
    "                v_feat = v_feat / v_feat.norm(p=2, dim=-1, keepdim=True)\n",
    "                t_feat = t_feat / t_feat.norm(p=2, dim=-1, keepdim=True)\n",
    "                \n",
    "                v_emb, t_emb = v_feat.cpu().numpy(), t_feat.cpu().numpy()\n",
    "                \n",
    "        return v_emb, t_emb\n",
    "        \n",
    "\n",
    "def calculate_metrics(v_emb, t_emb):\n",
    "    if v_emb.ndim == 3: v_emb = v_emb.squeeze(1)\n",
    "    if t_emb.ndim == 3: t_emb = t_emb.squeeze(1)\n",
    "    \n",
    "    sim_matrix = cosine_similarity(v_emb, t_emb)\n",
    "    \n",
    "    ranks = []\n",
    "    for i in range(len(sim_matrix)):\n",
    "        sorted_indices = np.argsort(sim_matrix[i])[::-1]\n",
    "        rank = np.where(sorted_indices == i)[0][0] + 1\n",
    "        ranks.append(rank)\n",
    "        \n",
    "    ranks = np.array(ranks)\n",
    "    \n",
    "    print(\"Итоговые метрики\")\n",
    "    print(f\"Recall@1:  {np.mean(ranks == 1) * 100:.2f}%\")\n",
    "    print(f\"Recall@5:  {np.mean(ranks <= 5) * 100:.2f}%\")\n",
    "    print(f\"Recall@10: {np.mean(ranks <= 10) * 100:.2f}%\")\n",
    "    print(f\"Median Rank: {np.median(ranks)}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def generate_embeddings(pkl_path, save_path, model_name, num_frames=8, multi_captions=False):\n",
    "    df_full = pd.read_pickle(pkl_path)\n",
    "    df = df_full.copy()\n",
    "    if multi_captions:\n",
    "        df[\"caption\"] = df[\"caption\"].apply(lambda captions: max(captions, key=len))\n",
    "    \n",
    "    engine = UniversalVideoModel(model_type=model_name, num_frames=num_frames)\n",
    "\n",
    "    emb_id = []\n",
    "    video_paths = []\n",
    "    captions = []\n",
    "    categories = []\n",
    "    v_embeddings = []\n",
    "    t_embeddings = []\n",
    "    \n",
    "    print(\"Старт инференса\")\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        if idx == 100:\n",
    "            break\n",
    "        v, t = engine.encode(row['video_path'], row['caption'])\n",
    "        if v is not None:\n",
    "            emb_id.append(idx)\n",
    "            video_paths.append(row['video_path'])\n",
    "            captions.append(row['caption'])\n",
    "            categories.append(row['category'])\n",
    "            v_embeddings.append(v)\n",
    "            t_embeddings.append(t)\n",
    "            \n",
    "            \n",
    "    v_emb_all = np.vstack(v_embeddings)\n",
    "    t_emb_all = np.vstack(t_embeddings)\n",
    "    calculate_metrics(v_emb_all, t_emb_all)\n",
    "\n",
    "    res = pd.DataFrame({\n",
    "        \"video_path\": video_paths,\n",
    "        \"caption\": captions,\n",
    "        \"category\": categories,\n",
    "        \"video_emb\": v_embeddings,\n",
    "        \"text_emb\": t_embeddings\n",
    "    }, index=emb_id)\n",
    "\n",
    "    res.to_pickle(save_path)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_path</th>\n",
       "      <th>caption</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSRVTT_videos/video9778.mp4</td>\n",
       "      <td>a little boy singing in front of judges and crowd</td>\n",
       "      <td>kids/family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSRVTT_videos/video9832.mp4</td>\n",
       "      <td>a video game character rides around on a motor...</td>\n",
       "      <td>vehicles/autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MSRVTT_videos/video7767.mp4</td>\n",
       "      <td>a slideshow with captions</td>\n",
       "      <td>music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MSRVTT_videos/video7369.mp4</td>\n",
       "      <td>a man is talking about opening a laptop case</td>\n",
       "      <td>howto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MSRVTT_videos/video9731.mp4</td>\n",
       "      <td>a woman is mixing food in a mixing bowl</td>\n",
       "      <td>cooking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>MSRVTT_videos/video8814.mp4</td>\n",
       "      <td>the judges make a decision</td>\n",
       "      <td>kids/family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>MSRVTT_videos/video9827.mp4</td>\n",
       "      <td>lady gaga sings in a music video</td>\n",
       "      <td>music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>MSRVTT_videos/video9815.mp4</td>\n",
       "      <td>a mashup of music videos is being played</td>\n",
       "      <td>music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>MSRVTT_videos/video8901.mp4</td>\n",
       "      <td>men pushing a car down assembly line</td>\n",
       "      <td>vehicles/autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>MSRVTT_videos/video8322.mp4</td>\n",
       "      <td>a lady tries to ride on bicycle but fails</td>\n",
       "      <td>vehicles/autos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      video_path  \\\n",
       "0    MSRVTT_videos/video9778.mp4   \n",
       "1    MSRVTT_videos/video9832.mp4   \n",
       "2    MSRVTT_videos/video7767.mp4   \n",
       "3    MSRVTT_videos/video7369.mp4   \n",
       "4    MSRVTT_videos/video9731.mp4   \n",
       "..                           ...   \n",
       "994  MSRVTT_videos/video8814.mp4   \n",
       "995  MSRVTT_videos/video9827.mp4   \n",
       "996  MSRVTT_videos/video9815.mp4   \n",
       "997  MSRVTT_videos/video8901.mp4   \n",
       "998  MSRVTT_videos/video8322.mp4   \n",
       "\n",
       "                                               caption        category  \n",
       "0    a little boy singing in front of judges and crowd     kids/family  \n",
       "1    a video game character rides around on a motor...  vehicles/autos  \n",
       "2                            a slideshow with captions           music  \n",
       "3         a man is talking about opening a laptop case           howto  \n",
       "4              a woman is mixing food in a mixing bowl         cooking  \n",
       "..                                                 ...             ...  \n",
       "994                         the judges make a decision     kids/family  \n",
       "995                   lady gaga sings in a music video           music  \n",
       "996           a mashup of music videos is being played           music  \n",
       "997               men pushing a car down assembly line  vehicles/autos  \n",
       "998          a lady tries to ride on bicycle but fails  vehicles/autos  \n",
       "\n",
       "[999 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle(\"MSRVTT_videos/msrvtt_dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка модели: XCLIP...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccfafcb2ec734254a385180a3030d3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/310 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296bdf3d17eb4e858665b437d6c3ed7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/927 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b8e08a25544040802fd1368ef49714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539a2c50d5804c22905d8a1baee7ae3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba89fc8cf0ec483b86e3f44a04e38857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe277548bc04fbc90ad46463206a4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8850882ac1734bbcb5a9aefe4101331b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262e34981ec14325835c569451eb03d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.30G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель загружена\n",
      "Старт инференса\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                          | 0/999 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpkl_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMSRVTT_videos/msrvtt_dataset.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMSRVTT_videos/xclip_embeddings.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxclip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_captions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 136\u001b[0m, in \u001b[0;36mgenerate_embeddings\u001b[0;34m(pkl_path, save_path, model_name, num_frames, multi_captions)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m100\u001b[39m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m v, t \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvideo_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcaption\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     emb_id\u001b[38;5;241m.\u001b[39mappend(idx)\n",
      "Cell \u001b[0;32mIn[16], line 73\u001b[0m, in \u001b[0;36mUniversalVideoModel.encode\u001b[0;34m(self, video_path, text)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxclip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     66\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor(\n\u001b[1;32m     67\u001b[0m         text\u001b[38;5;241m=\u001b[39m[text], \n\u001b[1;32m     68\u001b[0m         videos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(frames), \n\u001b[1;32m     69\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     70\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     71\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 73\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     v_emb \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mvideo_embeds\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     76\u001b[0m     t_emb \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mtext_embeds\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/clipteka/baseline/video-retrieval-baseline/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/clipteka/baseline/video-retrieval-baseline/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/clipteka/baseline/video-retrieval-baseline/venv/lib/python3.11/site-packages/transformers/models/x_clip/modeling_x_clip.py:1435\u001b[0m, in \u001b[0;36mXCLIPModel.forward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, return_loss, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m   1430\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1431\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1432\u001b[0m )\n\u001b[1;32m   1433\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1435\u001b[0m batch_size, num_frames, num_channels, height, width \u001b[38;5;241m=\u001b[39m \u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[1;32m   1436\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_channels, height, width)\n\u001b[1;32m   1438\u001b[0m vision_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_model(\n\u001b[1;32m   1439\u001b[0m     pixel_values\u001b[38;5;241m=\u001b[39mpixel_values,\n\u001b[1;32m   1440\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1443\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1444\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "generate_embeddings(\n",
    "    pkl_path=\"MSRVTT_videos/msrvtt_dataset.pkl\",\n",
    "    save_path=\"MSRVTT_videos/xclip_embeddings.pkl\",\n",
    "    model_name=\"xclip\",\n",
    "    num_frames=8,\n",
    "    multi_captions=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_embeddings(\n",
    "    pkl_path=\"MSRVTT_videos/msrvtt_dataset.pkl\",\n",
    "    save_path=\"MSRVTT_videos/siglip_embeddings.pkl\",\n",
    "    model_name=\"siglip\",\n",
    "    num_frames=8,\n",
    "    multi_captions=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'caption'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpkl_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVATEX_videos/vatex_dataset.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVATEX_videos/xclip_embeddings.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxclip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_captions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 121\u001b[0m, in \u001b[0;36mgenerate_embeddings\u001b[0;34m(pkl_path, save_path, model_name, num_frames, multi_captions)\u001b[0m\n\u001b[1;32m    119\u001b[0m df \u001b[38;5;241m=\u001b[39m df_full\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_captions:\n\u001b[0;32m--> 121\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcaption\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m captions: \u001b[38;5;28mmax\u001b[39m(captions, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m))\n\u001b[1;32m    123\u001b[0m engine \u001b[38;5;241m=\u001b[39m UniversalVideoModel(model_type\u001b[38;5;241m=\u001b[39mmodel_name, num_frames\u001b[38;5;241m=\u001b[39mnum_frames)\n\u001b[1;32m    125\u001b[0m emb_id \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/clipteka/baseline/video-retrieval-baseline/venv/lib/python3.11/site-packages/pandas/core/frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4113\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4115\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/clipteka/baseline/video-retrieval-baseline/venv/lib/python3.11/site-packages/pandas/core/indexes/range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'caption'"
     ]
    }
   ],
   "source": [
    "generate_embeddings(\n",
    "    pkl_path=\"VATEX_videos/vatex_dataset.pkl\",\n",
    "    save_path=\"VATEX_videos/xclip_embeddings.pkl\",\n",
    "    model_name=\"xclip\",\n",
    "    num_frames=8,\n",
    "    multi_captions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_embeddings(\n",
    "    pkl_path=\"VATEX_videos/vatex_dataset.pkl\",\n",
    "    save_path=\"VATEX_videos/siglip_embeddings.pkl\",\n",
    "    model_name=\"siglip\",\n",
    "    num_frames=8,\n",
    "    multi_captions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка модели: XCLIP...\n",
      "Модель загружена\n",
      "Старт инференса\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                         | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpkl_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYouCook2_videos/youcook2_dataset.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYouCook2_videos/xclip_embeddings.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxclip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_captions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 136\u001b[0m, in \u001b[0;36mgenerate_embeddings\u001b[0;34m(pkl_path, save_path, model_name, num_frames, multi_captions)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m100\u001b[39m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m v, t \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvideo_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcaption\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     emb_id\u001b[38;5;241m.\u001b[39mappend(idx)\n",
      "Cell \u001b[0;32mIn[16], line 73\u001b[0m, in \u001b[0;36mUniversalVideoModel.encode\u001b[0;34m(self, video_path, text)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxclip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     66\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor(\n\u001b[1;32m     67\u001b[0m         text\u001b[38;5;241m=\u001b[39m[text], \n\u001b[1;32m     68\u001b[0m         videos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(frames), \n\u001b[1;32m     69\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     70\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     71\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 73\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     v_emb \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mvideo_embeds\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     76\u001b[0m     t_emb \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mtext_embeds\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/clipteka/baseline/video-retrieval-baseline/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/clipteka/baseline/video-retrieval-baseline/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/clipteka/baseline/video-retrieval-baseline/venv/lib/python3.11/site-packages/transformers/models/x_clip/modeling_x_clip.py:1435\u001b[0m, in \u001b[0;36mXCLIPModel.forward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, return_loss, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m   1430\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1431\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1432\u001b[0m )\n\u001b[1;32m   1433\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1435\u001b[0m batch_size, num_frames, num_channels, height, width \u001b[38;5;241m=\u001b[39m \u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[1;32m   1436\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_channels, height, width)\n\u001b[1;32m   1438\u001b[0m vision_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_model(\n\u001b[1;32m   1439\u001b[0m     pixel_values\u001b[38;5;241m=\u001b[39mpixel_values,\n\u001b[1;32m   1440\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1443\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1444\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "generate_embeddings(\n",
    "    pkl_path=\"YouCook2_videos/youcook2_dataset.pkl\",\n",
    "    save_path=\"YouCook2_videos/xclip_embeddings.pkl\",\n",
    "    model_name=\"xclip\",\n",
    "    num_frames=8,\n",
    "    multi_captions=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка модели: SIGLIP...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nSiglipTokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpkl_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYouCook2_videos/youcook2_dataset.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYouCook2_videos/siglip_embeddings.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msiglip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_captions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 123\u001b[0m, in \u001b[0;36mgenerate_embeddings\u001b[0;34m(pkl_path, save_path, model_name, num_frames, multi_captions)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_captions:\n\u001b[1;32m    121\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m captions: \u001b[38;5;28mmax\u001b[39m(captions, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m))\n\u001b[0;32m--> 123\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[43mUniversalVideoModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m emb_id \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    126\u001b[0m video_paths \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[16], line 29\u001b[0m, in \u001b[0;36mUniversalVideoModel.__init__\u001b[0;34m(self, model_type, num_frames)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msiglip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     28\u001b[0m     model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/siglip-so400m-patch14-384\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor \u001b[38;5;241m=\u001b[39m \u001b[43mAutoProcessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/clipteka/baseline/video-retrieval-baseline/venv/lib/python3.11/site-packages/transformers/models/auto/processing_auto.py:396\u001b[0m, in \u001b[0;36mAutoProcessor.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m processor_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    393\u001b[0m         pretrained_model_name_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    394\u001b[0m     )\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m processor_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprocessor_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# Last try: we use the PROCESSOR_MAPPING.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m PROCESSOR_MAPPING:\n",
      "File \u001b[0;32m~/clipteka/baseline/video-retrieval-baseline/venv/lib/python3.11/site-packages/transformers/processing_utils.py:1394\u001b[0m, in \u001b[0;36mProcessorMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1392\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token\n\u001b[0;32m-> 1394\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_arguments_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1395\u001b[0m processor_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_processor_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_args_and_dict(args, processor_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/clipteka/baseline/video-retrieval-baseline/venv/lib/python3.11/site-packages/transformers/processing_utils.py:1453\u001b[0m, in \u001b[0;36mProcessorMixin._get_arguments_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1451\u001b[0m         attribute_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_possibly_dynamic_module(class_name)\n\u001b[0;32m-> 1453\u001b[0m     args\u001b[38;5;241m.\u001b[39mappend(\u001b[43mattribute_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1455\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m~/clipteka/baseline/video-retrieval-baseline/venv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:1156\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1154\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1155\u001b[0m         )\n\u001b[0;32m-> 1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m~/clipteka/baseline/video-retrieval-baseline/venv/lib/python3.11/site-packages/transformers/utils/import_utils.py:2157\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_dummy\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmro\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 2157\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/clipteka/baseline/video-retrieval-baseline/venv/lib/python3.11/site-packages/transformers/utils/import_utils.py:2143\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   2140\u001b[0m         failed\u001b[38;5;241m.\u001b[39mappend(msg\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[1;32m   2142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 2143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nSiglipTokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "generate_embeddings(\n",
    "    pkl_path=\"YouCook2_videos/youcook2_dataset.pkl\",\n",
    "    save_path=\"YouCook2_videos/siglip_embeddings.pkl\",\n",
    "    model_name=\"siglip\",\n",
    "    num_frames=8,\n",
    "    multi_captions=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ssl-baseline",
   "language": "python",
   "name": "ssl-baseline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
